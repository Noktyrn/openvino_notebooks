{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INT8 Quantization by POT in Simplified Mode tutorial\n",
    "\n",
    "This tutorial shows how to quantize a ResNet20 image classification model, trained on CIFAR10 dataset, using the Simplified Mode of OpenVINO's Post-Training Optimization Tool. \n",
    "\n",
    "Simplified mode is designed to make data preparation for the model optimization process easier. The mode is represented by an implementation of Engine interface from the POT API. It allows reading the data from an arbitrary folder specified by the user. Currently, Simplified mode is available only for image data in PNG or JPEG formats, stored in a single folder.\n",
    "\n",
    "Note: This mode cannot be used with accuracy-aware methods. There is no way to control accuracy after optimization. Nevertheless, this mode can be helpful to estimate performance benefits when using model optimizations.\n",
    "\n",
    "This tutorial has the following steps:\n",
    "\n",
    "- Downloading and saving the CIFAR10 dataset\n",
    "- Preparing the model for quantization\n",
    "- Compressing the prepared model\n",
    "- Measuring and comparing the performance of the original and quantized models\n",
    "- Demonstrating the use of the quantized model for image classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the data and model directories\n",
    "MODEL_DIR = 'model'\n",
    "CALIB_DIR = 'calib'\n",
    "CIFAR_DIR = 'cifar'\n",
    "CALIB_SET_SIZE = 300\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(CALIB_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the calibration dataset\n",
    "To prepare the calibration dataset we need to do the following:\n",
    "- Download CIFAR10 dataset from Torchvision.datasets repository\n",
    "- Save the selected number of elements from this dataset as .png images to the separate folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.ToTensor()])\n",
    "dataset = CIFAR10(root=CIFAR_DIR, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_converter = T.ToPILImage(mode=\"RGB\")\n",
    "\n",
    "for idx, info in enumerate(dataset):\n",
    "    im = info[0]\n",
    "    if idx >= CALIB_SET_SIZE:\n",
    "        break\n",
    "    label = info[1]\n",
    "    pil_converter(im.squeeze(0)).save(Path(CALIB_DIR) / f'{label}_{idx}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Model\n",
    "Model preparation stage includes the following steps:,\n",
    "- Download PyTorch model from Torchvision repository,\n",
    "- Convert it to ONNX format,\n",
    "- Run OpenVINO Model Optimizer tool to convert ONNX to OpenVINO Intermediate Representation (IR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_resnet20\", pretrained=True)\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "onnx_model_path = Path(MODEL_DIR) / 'resnet20.onnx'\n",
    "ir_model_xml = onnx_model_path.with_suffix('.xml')\n",
    "ir_model_bin = onnx_model_path.with_suffix('.bin')\n",
    "\n",
    "torch.onnx.export(model, dummy_input, onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert this model into the OpenVINO IR using the Model Optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mo --framework=onnx --data_type=FP32 --input_shape=[1,3,32,32] -m $onnx_model_path  --output_dir $MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression stage\n",
    "Model compression can be performed by calling the following command:\n",
    "  \n",
    "`pot -q default -m <path_to_xml> -w <path_to_bin> --engine simplified --data-source <path_to_data>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pot -q default -m $ir_model_xml -w $ir_model_bin --engine simplified --data-source $CALIB_DIR --output-dir compressed --direct-dump "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance of the Original and Quantized Models\n",
    "\n",
    "Finally, we will measure the inference performance of the FP32 and INT8 models. To do this, we use [Benchmark Tool](https://docs.openvinotoolkit.org/latest/openvino_inference_engine_tools_benchmark_tool_README.html) - OpenVINO's inference performance measurement tool.\n",
    "\n",
    "NOTE: For more accurate performance, we recommended running benchmark_app in a terminal/command prompt after closing other applications. Run benchmark_app -m model.xml -d CPU to benchmark async inference on CPU for one minute. Change CPU to GPU to benchmark on GPU. Run benchmark_app --help to see an overview of all command line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model_path = Path('compressed/optimized')\n",
    "optimized_model_xml = optimized_model_path / 'resnet20.xml'\n",
    "optimized_model_bin = optimized_model_path / 'resnet20.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference FP32 model (IR)\n",
    "!benchmark_app -m $ir_model_xml -d CPU -api async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference INT8 model (IR)\n",
    "!benchmark_app -m $optimized_model_xml -d CPU -api async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of the results\n",
    "\n",
    "In this section the usage of the comressed model will be demonstrated. For this purpose we will run the optimized model on a selected number of pictures from the CIFAR10 dataset and show the predictions of this model.\n",
    "\n",
    "In the first step the network is loaded using the IECore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()\n",
    "\n",
    "# read and load quantized model\n",
    "quantized_net = ie.read_network(\n",
    "    model=optimized_model_xml, weights=optimized_model_bin\n",
    ")\n",
    "quantized_net = ie.load_network(network=quantized_net, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then all the pictures and their labels from the dataset are stored in the lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all possible labels from CIFAR10\n",
    "labels_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "all_pictures = []\n",
    "all_labels = []\n",
    "\n",
    "# get all pictures and their labels \n",
    "for batch in dataset:\n",
    "    all_pictures.append(batch[0])\n",
    "    all_labels.append(batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the function, which shows the pictures and their labels using the indexes and two lists formed on the previous step, is defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pictures(indexes: list, all_pictures=all_pictures, all_labels=all_labels):\n",
    "    \"\"\"Plot pictures with the specified indexes.\n",
    "    :param indexes: a list of indexes of pictures to be displayed.\n",
    "    :param all_batches: batches with pictures.\n",
    "    \"\"\"\n",
    "    num_pics = len(indexes)\n",
    "    f, axarr = plt.subplots(1, num_pics)\n",
    "    for idx, im_idx in enumerate(indexes):\n",
    "        assert idx < 10000, 'Cannot get such index, there are only 10000'\n",
    "        pic = np.rollaxis(all_pictures[im_idx].squeeze().numpy(), 0, 3)\n",
    "        axarr[idx].imshow(pic)\n",
    "        axarr[idx].set_title(labels_names[all_labels[im_idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define the function, which uses optimized model to get the predictions for the selected pictures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_on_pictures(net, indexes: list, all_pictures=all_pictures):\n",
    "    \"\"\" Inference model on a set of pictures.\n",
    "    :param net: model on which do inference\n",
    "    :param indexes: list of indexes \n",
    "    \"\"\"\n",
    "    predicted_labels = []\n",
    "    for idx in indexes:\n",
    "        assert idx < 10000, 'Cannot get such index, there are only 10000'\n",
    "        result = list(net.infer(inputs={'input.1': all_pictures[idx]}).values())\n",
    "        result = labels_names[np.argmax(result[0])]\n",
    "        predicted_labels.append(result)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_infer = [0, 1, 2]  # to plot specify indexes\n",
    "\n",
    "plot_pictures(indexes_to_infer)\n",
    "\n",
    "results_quanized = infer_on_pictures(quantized_net, indexes_to_infer)\n",
    "\n",
    "print(f\"Labels for picture from quantized model : {results_quanized}.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "K5HPrY_d-7cV",
    "E01dMaR2_AFL",
    "qMnYsGo9_MA8",
    "L0tH9KdwtHhV"
   ],
   "name": "INT8 Quantization by POT in Simplified Mode tutorial",
   "provenance": []
  },
  "interpreter": {
   "hash": "852430a53033d44ce17eefa3d9017e4bc4dca84b51e1c38a8c71aec2fd2e4d43"
  },
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
